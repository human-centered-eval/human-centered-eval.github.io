<!DOCTYPE html>
<html lang="en">
<title>Human-Centered Eval@EMNLP24</title>
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" href="https://unpkg.com/tachyons/css/tachyons.min.css">
<link rel="stylesheet" href="./style.css">

<script>
    document.addEventListener('DOMContentLoaded', function() {
    const readMoreBtn = document.querySelector('.read-more');
    const readLessBtn = document.querySelector('.read-less');
    const bioHidden = document.querySelector('.bio-hidden');

    readMoreBtn.addEventListener('click', function() {
        bioHidden.style.display = 'block';
        readMoreBtn.style.display = 'none';
    });

    readLessBtn.addEventListener('click', function() {
        bioHidden.style.display = 'none';
        readMoreBtn.style.display = 'inline';
    });
});
</script>

<body class="sans-serif">
    <header class="sans-serif">
        <div class="cover bg-left bg-center-l accent-bg" style="background-image: url('assets/banner.svg');">
            <div class="pb4 pb4-m pb4-l">
                <nav class="dt w-100 mw8 center">
                    <div class="dtc w6 v-mid pa1">
                        <a href="" class="f4 fw5 dib no-underline grow white pa2 grow">
                            Human-Centered Evaluation of Language Technologies
                        </a>
                    </div>
                    <div class="dtc v-mid tr pa3">
                        <!-- <a class="f6 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href="#cfp">Call for
                            Participation</a>
                        <a class="f6 fw4 hover-white no-underline white-70 dn dib-l pv2 ph3" href="#info">Key Information</a> -->
                        <a class="f6 fw4 hover-white no-underline white-70 dn dib-l pv2 ph3" href="#agenda">Agenda</a>
                        <a class="f6 fw4 hover-white no-underline white-70 dn dib-l pv2 ph3" href="#info">Instructors</a>
                    </div>
                </nav>
                <div class="tc-l mt4 mt5-m mt6-l ph5 w-75 center">
                    <h1 class="f2 f1-l fw6 white-90 mb0 lh-title">Human-Centered Evaluation of Language Technologies</h1>
                    <h2 class="fw3 f3 white-80 mt3 mb4">EMNLP 2024 Tutorial<br>Saturday, Nov 16, 14:00-17:30</h2>
                    <h3 class="fw3 f3 white-80 mt3 mb4">Miami, Florida, USA</h3>
                    <!-- <a class="f6 link grow br3 ba bw1 ph3 pv2 mb2 dib white" href="https://openreview.net/group?id=ACM.org/CHI/2024/Workshop/HEAL" target="_blank">→ Submission Site</a> -->
                </div>
            </div>
        </div>
    </header>

    <div class="text-section-padded">
        <article class="cf ph3 ph5-ns pv5" id="overview">
            <header class="fn fl-ns w-30-ns pr4-ns">
                <h1 class="f3 lh-title fw5 mb3 mt0 pt3 bt bw2 accent-txt">
                    Overview
                </h1>
            </header>
            <div class="fn fl-ns w-70-ns">
                <p class="f5 lh-copy mt0-ns">
                    Evaluation is a cornerstone topic in NLP. However, many criticisms have been raised about the community's evaluation practices, including a lack of human-centered considerations about people's needs for language technologies and technologies' actual impact on people. This “evaluation crisis” is exacerbated by the recent development of large generative models with diverse and uncertain capabilities. This tutorial aims to inspire more human-centered evaluation in NLP by introducing perspectives and methodologies from the social sciences and human-computer interaction (HCI), a field concerned primarily with the design and evaluation of technologies. The tutorial will start with an overview of current NLP evaluation practices and their limitations, then introduce complementary perspectives from the social sciences and a “toolbox of evaluation methods” from HCI, accompanied by discussions of considerations such as what to evaluate for, how generalizable the results are to the real-world contexts, and pragmatic costs of conducting the evaluation. The tutorial will also encourage reflection on how these HCI perspectives and methodologies can complement NLP evaluation through Q&A discussions and a hands-on exercise. 
                </p>
                <!-- <p class="f5 lh-copy">
                    The <b>CHI 2024 Workshop</b> on <b>H</b>uman-centered <b>E</b>valuation and <b>A</b>uditing of <b>L</b>anguage Models (<b>HEAL@CHI'24</b>) will explore topics around understanding stakeholders' needs and goals with evaluation and auditing LMs, establishing human-centered evaluation and auditing methods, developing tools and resources to support these methods, building community, and fostering collaboration.
                </p> -->
            </div>
        </article>

        <!-- <article class="cf ph3 ph5-ns pv5" id="keynote">
            <header class="fn fl-ns w-30-ns pr4-ns">
                <h1 class="f3 lh-title fw5 mb3 mt0 pt3 bt bw2 accent-txt">
                    Keynote Speakers
                </h1>
            </header>
            <div class="fn fl-ns w-70-ns">
                <div class="speaker-container">
                    <div class="speaker-photo">
                        <div class="aspect-ratio aspect-ratio--1x1">
                            <img src="/assets/wei.png" class="db bg-center cover aspect-ratio--object br-100" alt="Afternoon Keynote Speaker">
                        </div>
                    </div>

                    <div class="speaker-info">
                        <h2>Dr. Xu Wei</h2>
                        <h3>Human-AI Collaboration in Evaluating Large Language Models</h3>
                        <p>To support real-world applications more responsibly and further improve large language models (LLMs), it is essential to design reliable and reusable frameworks for their evaluation. In this talk, I will discuss three forms of human-AI collaboration for evaluation that combine the strengths of both: (1) the reliability and user-centric aspect of human evaluation, and (2) the cost efficiency and reproducibility offered by automatic evaluation. The first part focuses on systematically assessing LLMs’ favoritism towards Western culture, using a hybrid approach of manual effort and automated analysis. The second part will showcase an LLM-powered privacy preservation tool, designed to safeguard users against the disclosure of personal information. I will share some interesting findings from an HCI user study that involves real Reddit users utilizing our tool, which in turn informs our ongoing efforts to improve the design of NLP models. Lastly, we will delve into the evaluation of LLM-generated texts, where human judgments can be used to train automatic evaluation metrics to detect errors. We also highlight the opportunity of engaging both laypeople and experts in evaluating LLM-generated simplified medical texts in high-stake healthcare applications. </p>
                        <h3>Bio</h3>
                        <p>
                            Wei Xu is an Associate Professor in the College of Computing and Machine Learning Center at the Georgia Institute of Technology, where she is the director of the NLP X Lab. Her research interests are in natural language processing and machine learning, with a focus on Generative AI, robustness and fairness of large language models, multilingual LLMs, as well as AI for science, education, accessibility, and privacy research. She is a recipient of the NSF CAREER Award, CrowdFlower AI for Everyone Award, Best Paper Award and Honorable Mention at COLING'18, ACL'23. She also received research funds from DARPA and IARPA. She is currently an executive board member of NAACL. 
                        </p>
                    </div>
                </div>
                <div class="speaker-container">
                    <div class="speaker-photo">
                        <div class="aspect-ratio aspect-ratio--1x1">
                            <img src="/assets/sherry.png" class="db bg-center cover aspect-ratio--object br-100" alt="Afternoon Keynote Speaker">
                        </div>
                    </div>

                    <div class="speaker-info">
                        <h2>Dr. Sherry Tongshuang Wu</h2>
                        <h3>Practical AI Systems: From General-Purpose AI to (the Right) Specific Use Cases</h3>
                        <p>AI research has made great strides in developing general-purpose models (e.g., LLMs) that can excel across a wide range of tasks, enabling users to explore AI applications tailored to their unique needs without the complexities of custom model training. However, with the opportunities come the challenges — General-purpose models prioritize overall performance, but this can neglect specific user needs. How can we make these models practically usable?  In this talk, I will present our recent work on assessing and tailoring general-purpose models for specific use cases. I will first cover methods for evaluating and mapping LLMs to specific usage scenarios, then reflect on the importance of identifying the right tasks for LLMs by comparing how humans and LLMs may perform the same tasks differently. In my final remarks, I will discuss the potential of training humans and LLMs with complementary skill sets. </p>
                        <h3>Bio</h3>
                        <p>
                            Sherry Tongshuang Wu is an Assistant Professor in the Human-Computer Interaction Institute at Carnegie Mellon University. Her research lies at the intersection of Human-Computer Interaction and Natural Language Processing, and primarily focuses on how humans (AI experts, lay users, domain experts) can practically interact with (debug, audit, and collaborate with) AI systems. To this end, she has worked on assessing NLP model capabilities, supporting human-in-the-loop NLP model debugging and correction, as well as facilitating human-AI collaboration. She has authored award-winning papers in top-tier NLP, HCI and Visualization conferences and journals such as ACL, CHI, TOCHI, TVCG, etc. Before joining CMU, Sherry received her Ph.D. degree from the University of Washington and her bachelor degree from the Hong Kong University of Science and Technology, and has interned at Microsoft Research, Google Research, and Apple. You can find out <a href="http://cs.cmu.edu/~sherryw">more</a> about her.
                        </p>
                    </div>
                </div>
            </div>

            </div>
        </article> -->

        <article class="cf ph3 ph5-ns pv5" id="agenda">
            <header class="fn fl-ns w-30-ns pr4-ns">
                <h1 class="f3 lh-title fw5 mb3 mt0 pt3 bt bw2 accent-txt">
                    Agenda
                </h1>
            </header>
            <div class="fn fl-ns w-70-ns">
                <!-- <p class="f5 lh-copy mt0-ns">
                    The primary goal of this one-day workshop is to bring together HCI and AI researchers from academia, industry, and non-profits to share their ongoing efforts around evaluating and auditing language models.
                </p> -->
                <ul class="agenda-list">
                    <li>
                        <h3>Motivation and Overview</h3>
                    </li>
                    <li>
                        <h3>Current Evaluation Practices in NLP</h3>
                        <p>Overview of Different Types of NLP Evaluation</p>
                        <p>Concerns and Limitations</p>
                    </li>
                    <li>
                        <h3>Evaluating Evaluations: Perspectives from the Social Sciences</h3>
                    </li>
                    <li>
                        <h3>Human-Centered Evaluation Methods in HCI</h3>
                    </li>
                    <li>
                        <h3>Example Evaluation of Language Technologies in HCI Research</h3>
                        <p>Evaluating Writing Assistance</p>
                        <p>Evaluating Chatbot</p>
                    </li>
                    <li>
                        <h3>Reflection, Conclusion and Future Directions</h3>
                    </li>
                    <li>
                        <h3>Hands-on Group Exercise</h3>
                    </li>
                </ul>
            </div>
        </article>

        <!-- <article class="cf ph3 ph5-ns pv5" id="accepted">
            <header class="fn fl-ns w-30-ns pr4-ns">
                <h1 class="f3 lh-title fw5 mb3 mt0 pt3 bt bw2 accent-txt">
                    Accepted Work
                </h1>
            </header>
            <div class="fn fl-ns w-70-ns">
                <p class="f5 lh-copy mt0-ns">
                    <h3>Workshop papers</h4>
                    <ul>
                        <li class="pv1"><b>The Impossibility of Fair LLMs</b> - Jacy Reese Anthis, Kristian Lum, Michael Ekstrand, Avi Feller, Alexander D'Amour, Chenhao Tan <a href = "papers/1_the_impossibility_of_fair_LLMs.pdf">[Paper]</a></li>
                        <li class="pv1"><b>Evaluation of an LLM in Identifying Logical Fallacies</b> - Gionnieve Lim, Simon Perrault <a href = "papers/2_evaluation_of_an_llm_in_identi.pdf">[Paper]</a></li>
                        <li class="pv1"><b>Prediction-Powered Ranking of Large Language Models</b> - Ivi Chatzi, Eleni Straitouri, Suhas Thejaswi, Manuel Gomez Rodriguez <a href = "papers/3_prediction_powered_ranking_of_.pdf">[Paper]</a></li>
                        <li class="pv1"><b>A Framework for Evaluating Harms from Design Patterns in Human-AI Interfaces</b> - Lujain Ibrahim, Luc Rocher, Ana Valdivia <a href = "papers/4_evaluating_harms_from_design_p.pdf">[Paper]</a></li>
                        <li class="pv1"><b>Evaluating Irrationality in Large Language Models and Open Research Questions</b> - Dana R Alsagheer, Rabimba Karanjai, Weidong Shi, Nour Diallo, Yang Lu, Suha Beydoun, Qiaoning Zhang <a href = "papers/6_evaluating_irrationality_in_la.pdf">[Paper]</a></li>
                        <li class="pv1"><b>Evaluating Large Language Models: Stigma and Opioid Use Disorder</b> - Shravika Mittal, Mai ElSherief, Tanu Mitra, Munmun De Choudhury <a href = "papers/7_evaluating_large_language_mode.pdf">[Paper]</a></li>
                        <li class="pv1"><b>Who Validates the Validators? Aligning LLM-Assisted Evaluation of LLM Outputs with Human Preferences</b> - Shreya Shankar, J.D. Zamfirescu-Pereira, Aditya Parameswaran, Ian Arawjo </li>
                        <li class="pv1"><b>AffirmativeAI: Towards LGBTQ+ Friendly Audit Frameworks for Large Language Models</b> - Yinru Long, Zilin Ma, Yiyang Mei, Zhaoyuan Su <a href = "papers/11_affirmativeai_towards_lgbtq_fr.pdf">[Paper]</a></li>
                        <li class="pv1"><b>A Case for Moving Beyond "Gold Data" in AI Safety Evaluation</b> - Mark Diaz, Ding Wang, Alicia Parrish, Lora Aroyo, Christopher M Homan, Gregory Serapio-García, Vinodkumar Prabhakaran, Alex Taylor <a href = "papers/12_a_case_for_moving_beyond_gold_.pdf">[Paper]</a><b style = "color:white ;background :#C2A84D">Highlight</b></li>
                        <li class="pv1"><b>Does GPT Distrust Algorithms? Evaluating Large Language Models for Algorithm Aversion</b> - Jessica Bo, Lillio Mok, Jiessie Tie, Ashton Anderson <a href = "papers/13_does_gpt_distrust_algorithms_.pdf">[Paper]</a></li>
                        <li class="pv1"><b>Exploring Bengali Religious Dialect Biases in Large Language Models with Evaluation Perspectives</b> - Azmine Toushik Wasi, Raima Islam, Mst Rafia Islam, Taki Hasan Rafi, Dong-Kyu Chae </li>
                        <li class="pv1"><b>Prompt Templates: A Methodology for Improving Manual Red Teaming Performance</b> - Brandon Dominique, David Piorkowski, Ioana Baldini, Manish Nagireddy </li>
                        <li class="pv1"><b>Exploring Subjectivity for more Human-Centric Assessment of Biases in Large Language Models</b> - Paula Akemi Aoyagui, Sharon Ferguson, Anastasia Kuzminykh <a href = "papers/21_exploring_subjectivity_for_mor.pdf">[Paper]</a></li>
                        <li class="pv1"><b>Towards a Holistic Evaluation of LLM Generated Code for Exploratory Visual Analysis</b> - Anamaria Crisan, Enamul Hoque <a href = "papers/22_towards_a_holistic_evaluation_.pdf">[Paper]</a><b style = "color:white ;background :#C2A84D">Highlight </b></li>
                        <li class="pv1"><b>Exploring the Potential of the Large Language Models (LLMs) in Identifying and Explaining Misleading News Headlines</b> - Md Main Uddin Rony, Md Mahfuzul Haque, Mohammad Ali, Ahmed Shatil Alam, Naeemul Hassan <a href = "papers/23_exploring_the_potential_of_the.pdf">[Paper]</a></li>
                        <li class="pv1"><b>Tales from the Wild West: Crafting Scenarios to Audit Bias in LLMs</b> - Katherine-Marie Robinson, Violet Turri, Shannon K Gallagher, Carol J Smith <a href = "papers/24_tales_from_the_wild_west_craft.pdf">[Paper]</a></li>
                        <li class="pv1"><b>Designing an open-source LLM interface and social platforms for collectively driven LLM evaluation and auditing</b> - Jaeryang Baek, Nicholas Vincent, Lawrence H Kim <b style = "color:white ;background :#C2A84D"> Highlight </b></li>
                        <li class="pv1"><b>Lessons from Developing and Evaluating LLMs for Data Visualization</b> - Qianwen Wang <a href = "papers/29_lessons_from_developing_and_ev.pdf">[Paper]</a></li>
                        <li class="pv1"><b>Towards an Evaluation of LLM-Generated Inspiration by Developing and Validating Inspiration Scale</b> - Hyungyu Shin, Seulgi choi, Ji Yong Cho, Sahar Admoni, Hyunseung Lim, Taewan Kim, Hwajung Hong, Moontae Lee, Juho Kim <a href = "papers/30_towards_an_evaluation_of_llm_g.pdf">[Paper]</a></li>
                        <li class="pv1"><b>CheckEval: Robust Evaluation Framework using Large Language Model via Checklist</b> - Yukyung Lee, JoongHoon Kim, Jaehee Kim, Hyowon Cho, Pilsung Kang <a href = "papers/31_checkeval_robust_evaluation_fr.pdf">[Paper]</a></li>
                        <li class="pv1"><b>Identifying and Categorizing Harms in Human-AI Social Interactions</b> - Renwen Zhang, Han Li, Jinyuan Zhan, Hongyuan Gan </li>
                        <li class="pv1"><b>How to Reflect Diverse People's Perspectives in Large-Scale LLM-based Evaluations?</b> - Yoonjoo Lee, Tae Soo Kim, Juho Kim <a href = "papers/34_how_to_reflect_diverse_people_.pdf">[Paper]</a></li>
                        <li class="pv1"><b>Evaluating the LLM Agents for Simulating Humanoid Behavior</b> - Chaoran Chen, Bingsheng Yao, Yanfang Ye, Dakuo Wang, Toby Jia-Jun Li <a href = "papers/35_evaluating_the_llm_agents_for_.pdf">[Paper]</a></li>
                        <li class="pv1"><b>Validation Without Ground Truth? Methods for Trust in Generative Simulations</b> - Helena Vasconcelos, Carolyn Zou, Lindsay Popowski, Tobias Gerstenberg, Ranjay Krishna, Michael S. Bernstein <b style = "color:white ;background :#C2A84D"> Highlight </b></li>
                        <li class="pv1"><b>Involving Affected Communities and Their Knowledge for Bias Evaluation in Large Language Models</b> - Vildan Salikutluk, Elifnur Dogan, Isabelle Clev, Frank Jäkel <a href = "papers/38_involving_affected_communities.pdf">[Paper]</a></li>
                        <li class="pv1"><b>Metrics to Meaning: Enabling Human-Interpretable Language Model Assessment</b> - Jay Oza, Hrishikesh Yadav <a href = "papers/39_metrics_to_meaning_enabling_hu.pdf">[Paper]</a></li>
                     </ul>

                    <h3>Encore papers</h4>
                    <ul><li class="pv1"><b>Finspector: A Human-Centered Visual Inspection Tool for Exploring and Comparing Biases among Foundation Models</b> - Bum Chul Kwon, Nandana Mihindukulasooriya <a href = "https://www.bckwon.com/publication/finspector/">[Link]</a></li>
                        <li class="pv1"><b>EvalLM: Interactive Evaluation of Large Language Model Prompts on User-Defined Criteria</b> - Tae Soo Kim, Yoonjoo Lee, Jamin Shin, Young-Ho Kim, Juho Kim <a href = "papers/9_evallm_interactive_evaluation_.pdf">[Paper]</a></li> 
                        <li class="pv1"><b>Incorporating Multi-Stakeholder Perspectives in Evaluating and Auditing of Health Chatbots</b> - Eunkyung Jo, Young-Ho Kim, Yuin Jeong, SoHyun Park, Daniel Epstein <a href = "papers/10_incorporating_multi_stakeholde.pdf">[Paper]</a><b style = "color:white ;background :#C2A84D">Highlight</b></li>
                        <li class="pv1"><b>Wikibench: Community-Driven Data Curation for AI Evaluation on Wikipedia</b> - Tzu-Sheng Kuo, Aaron Halfaker, Zirui Cheng, Jiwoo Kim, Meng-Hsin Wu, Tongshuang Wu, Ken Holstein, Haiyi Zhu <a href = "papers/14_wikibench_community_driven_dat.pdf">[Paper]</a></li>                        
                        <li class="pv1"><b>Towards Designing a Safe and Reliable LLM-driven Chatbot for Children</b> - Woosuk Seo, Sun Young Park, Mark Ackerman, Chan-Mo Yang, Young-Ho Kim <a href = "papers/20_towards_designing_a_safe_and_r.pdf">[Paper]</a></li>
                        <li class="pv1"><b>Evaluating and Auditing LLM-Driven Chatbots for Psychiatric Patients in Clinical Mental Health Settings</b> - Taewan Kim, Seolyeong, Hyun Ah Kim, Su-woo Lee, Hwajung Hong, Chanmo Yang, Young-Ho Kim <a href = "papers/25_evaluating_and_auditing_llm_dr.pdf">[Paper]</a></li>
                        <li class="pv1"><b>LLM Comparator: Visual Analytics for Side-by-Side Evaluation of Large Language Models</b> - Minsuk Kahng, Ian Tenney, Mahima Pushkarna, Michael Xieyang Liu, James Wexler, Emily Reif, Krystal Kallarackal, Minsuk Chang, Michael Terry, Lucas Dixon <a href = "papers/26_llm_comparator_visual_analytic.pdf">[Paper]</a></li>
                        <li class="pv1"><b>"It's the only thing I can trust": Envisioning Large Language Model Use by Autistic Workers for Communication Assistance</b> - JiWoong Jang, Sanika Moharana, Patrick Carrington, Andrew Begel <a href = "papers/28__it_s_the_only_thing_i_can_tru.pdf">[Paper]</a></li>
                    </ul>
                </p>
            </div>
        </article>

        <article class="cf ph3 ph5-ns pv5" id="info">
            <header class="fn fl-ns w-30-ns pr4-ns">
                <h1 class="f3 lh-title fw5 mb3 mt0 pt3 bt bw2 accent-txt">
                    Key Information
                </h1>
            </header>
            <div class="fn fl-ns w-70-ns">
                <p><b>Submission deadline</b>: <del>Feb 23, 2024 (AoE)</del> Mar 1, 2024 (AoE)</p>
                <p><b>Notification of acceptance</b>: <del>Mar 22, 2024</del> Mar 24, 2024</p>
                <p><b>Workshop date</b>: Sunday, May 12, 2024</p>
                <p><b>Workshop location</b>: Honolulu, Hawaii, USA (Hybrid)</a></p>
                <p><b>Contact</b>: heal.workshop@gmail.com</a></p>
            </div>
        </article>

        <article class="cf ph3 ph5-ns pv5" id="cfp">
            <header class="fn fl-ns w-30-ns pr4-ns">
                <h1 class="f3 lh-title fw5 mb3 mt0 pt3 bt bw2 accent-txt">
                    Call for Participation
                </h1>
            </header>
            <div class="fn fl-ns w-60-ns">
                <p class="f5 lh-copy mt0-ns">
                    We welcome participants who work on topics related to <b>supporting human-centered evaluation and auditing of language models</b>. Interested participants will be asked to contribute a short paper to the workshop. Topics of interest include, but not limited to:

                    <ul>
                        <li class="pv1">Empirical understanding of stakeholders' needs and goals of LLM evaluation and auditing</li>
                        <li class="pv1">Human-centered evaluation and auditing methods for LLMs</li>
                        <li class="pv1">Tools, processes, and guidelines for LLM evaluation and auditing</li>
                        <li class="pv1">Discussion of regulatory measures and public policies for LLM auditing</li>
                        <li class="pv1">Ethics in LLM evaluation and auditing</li>
                    </ul>
                </p>
                <p class="f5 lh-copy mt0-ns">
                    <p class="pv1"><a style="font-weight: 700;">Submission Format:</a> 2 - 6 pages ACM double-column, excluding references.</p>
                    <p class="pv1"><a style="font-weight: 700;">Submission Types:</a> Position papers, full or in-progress empirical studies, literature reviews, system demos, method descriptions, or encore of published work. The submission will be non-archival. </p>
                    <p class="pv1"><a style="font-weight: 700;">Review Process:</a> Double-blind. Papers will be selected based on the quality of the submission and diversity of perspectives to allow for a meaningful exchange of knowledge between a broad range of stakeholders. </p>
                    <p><a style="font-weight: 700;">Templates: </a> <a class="accent-txt" href="https://www.acm.org/binaries/content/assets/publications/word_style/interim-template-style/interim-layout.docx">[Word]</a> <a class="accent-txt" href="https://portalparts.acm.org/hippo/latex_templates/acmart-primary.zip">[LaTex]</a> <a class="accent-txt" href="https://www.overleaf.com/latex/templates/acm-conference-proceedings-primary-article-template/wbvnghjbzwpc">[Overleaf]</a></p>
                    <p><a style="font-weight: 700;">Notes: </a>                    <ul>
                        <li class="pv1">We encourage authors who submit also to help with the review process.</li>
                        <li class="pv1">For an encore submission, you do not need to anonymize the submission. Encore submissions will go through a jury review process. </li>
                        <li class="pv1">If you are using LaTex, please use \documentclass[manuscript,review,anonymous]{acmart} for submission. </li>
                        <li class="pv1">For camera ready, please use \documentclass[sigconf]{acmart}.</li>
                    </ul></p>
                </p>
                <p>
                    <a class="f6 link grow br3 ba bw1 ph3 pv2 mb2 dib accent-txt" href="https://openreview.net/group?id=ACM.org/CHI/2024/Workshop/HEAL" target="_blank">→ Submission Site</a>
                </p>
            </div>
        </article> -->

        <article class="cf ph3 ph5-ns pv5" id="info">
            <header class="fn fl-ns w-30-ns pr4-ns">
                <h1 class="f3 lh-title fw5 mb3 mt0 pt3 bt bw2 accent-txt">
                    Instructors
                </h1>
            </header>
            <div class="fn fl-ns w-70-ns">
                <section class="cf w-200 pa2-ns">
                    <div class="cf w-100 measure-wide">
                        <div class="fl w-third pa2">
                            <div class="aspect-ratio aspect-ratio--1x1">
                                <img src="./assets/blodgett.jpeg" class="db bg-center cover aspect-ratio--object br-100">
                            </div>
                            <a href="https://sblodgett.github.io/" class="ph2 ph0-ns pb3 link db" target="_blank">
                                <p class="f5-ns mb0 fw6 black-90">Su Lin Blodgett</p>
                                <p class="f6 fw4 mt2 black-60">Microsoft Research</p>
                            </a>
                        </div>
                        <div class="fl w-third pa2">
                            <div class="aspect-ratio aspect-ratio--1x1">
                                <img src="./assets/cheung.jpg" class="db bg-center cover aspect-ratio--object br-100">
                            </div>
                            <a href="https://www.cs.mcgill.ca/~jcheung/" class="ph2 ph0-ns pb3 link db" target="_blank">
                                <p class="f5-ns mb0 fw6 black-90">Jackie Chi Kit Cheung</p>
                                <p class="f6 fw4 mt2 black-60">McGill University</p>
                            </a>
                        </div>
                    </div>
                    <div class="w-100 measure-wide">
                        <div class="fl w-third pa2">
                            <div class="aspect-ratio aspect-ratio--1x1">
                                <img src="./assets/liao.jpg" class="db bg-center cover aspect-ratio--object br-100">
                            </div>
                            <a href="http://qveraliao.com/" class="ph2 ph0-ns pb3 link db" target="_blank">
                                <p class="f5-ns mb0 fw6 black-90">Q. Vera Liao</p>
                                <p class="f6 fw4 mt2 black-60">Microsoft Research</p>
                            </a>
                        </div>
                        <div class="fl w-third pa2">
                            <div class="aspect-ratio aspect-ratio--1x1">
                                <img src="./assets/xiao.jpg" class="db bg-center cover aspect-ratio--object br-100">
                            </div>
                            <a href="https://ziangxiao.com/" class="ph2 ph0-ns pb3 link db" target="_blank">
                                <p class="f5-ns mb0 fw6 black-90">Ziang Xiao</p>
                                <p class="f6 fw4 mt2 black-60">Johns Hopkins University</p>
                            </a>
                        </div>
                    </div>
                </section>
            </div>
        </article>
    </div>


    <footer class="pv4 ph3 ph5-m ph6-l mid-gray">
        <small class="f6 db tc">Human-Centered Evaluation of Language Technologies</small>
    </footer>


</body>

</html>